"""
DBSCAN ç©ºé—´èšç±»åˆ†æ
å¯¹ç«™ç‚¹è¿›è¡Œç©ºé—´èšç±»ï¼Œç»“åˆå®¢æµé‡è¯†åˆ«äº¤é€šçƒ­ç‚¹åŒºåŸŸ
"""
import os
import sys
import numpy as np
import django

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'backend'))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'transit_system.settings')
django.setup()

from django.db.models import Sum
from analysis.models import BusStation, StationFlowStats, ClusterResult


def run_dbscan(eps=0.008, min_samples=2, date=None):
    """
    æ‰§è¡Œ DBSCAN ç©ºé—´èšç±»åˆ†æ

    å‚æ•°:
        eps: float - DBSCAN é‚»åŸŸåŠå¾„ (ç»çº¬åº¦è·ç¦»ï¼Œçº¦ 0.01 â‰ˆ 1km)
        min_samples: int - æœ€å°æ ·æœ¬æ•°
        date: str|None - å¯é€‰æ—¥æœŸç­›é€‰ (å¦‚ '2025-05-01')

    è¿”å›:
        dict - èšç±»ç»Ÿè®¡ä¿¡æ¯
    """
    from sklearn.cluster import DBSCAN
    from sklearn.preprocessing import StandardScaler

    print("=" * 60)
    print("  DBSCAN ç©ºé—´èšç±»åˆ†æ")
    print("=" * 60)

    # ------------------------------------------------------------------
    # 1. è·å–ç«™ç‚¹æ•°æ®åŠå…¶å®¢æµé‡
    # ------------------------------------------------------------------
    stations = BusStation.objects.all()
    if stations.count() == 0:
        print("é”™è¯¯: æ•°æ®åº“ä¸­æ²¡æœ‰ç«™ç‚¹æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ ETL æµç¨‹")
        return None

    station_data = []
    for station in stations:
        flow_qs = StationFlowStats.objects.filter(station=station)
        if date:
            flow_qs = flow_qs.filter(date=date)
        total_flow = flow_qs.aggregate(total=Sum('total_flow'))['total'] or 0

        station_data.append({
            'station': station,
            'lon': station.longitude,
            'lat': station.latitude,
            'total_flow': total_flow,
        })

    print(f"å…±è·å– {len(station_data)} ä¸ªç«™ç‚¹")

    # ------------------------------------------------------------------
    # 2. æ„å»ºç‰¹å¾çŸ©é˜µ (ç»åº¦, çº¬åº¦)
    # ------------------------------------------------------------------
    coords = np.array([[s['lon'], s['lat']] for s in station_data])

    # æ£€æŸ¥åæ ‡æ˜¯å¦å…¨éƒ¨ç›¸åŒï¼ˆè¯´æ˜æœªæ›´æ–°åæ ‡ï¼‰
    if np.std(coords[:, 0]) < 0.001 and np.std(coords[:, 1]) < 0.001:
        print("è­¦å‘Š: æ‰€æœ‰ç«™ç‚¹åæ ‡å‡ ä¹ç›¸åŒï¼è¯·å…ˆè¿è¡Œ update_station_coords.py æ›´æ–°åæ ‡")
        return None

    # ------------------------------------------------------------------
    # 3. æ‰§è¡Œ DBSCAN èšç±»
    #    ä½¿ç”¨ haversine åº¦é‡éœ€è¦å¼§åº¦ï¼Œè¿™é‡Œç®€åŒ–ç”¨æ¬§æ°è·ç¦» (ç»çº¬åº¦å°èŒƒå›´è¿‘ä¼¼å¯ç”¨)
    # ------------------------------------------------------------------
    print(f"DBSCAN å‚æ•°: eps={eps}, min_samples={min_samples}")

    db = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')
    labels = db.fit_predict(coords)

    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    n_noise = list(labels).count(-1)
    print(f"èšç±»æ•°é‡: {n_clusters}")
    print(f"å™ªå£°ç‚¹æ•°: {n_noise}")

    # ------------------------------------------------------------------
    # 4. è®¡ç®—æ¯ä¸ªèšç±»çš„æ€»å®¢æµï¼Œè¯†åˆ«çƒ­ç‚¹
    # ------------------------------------------------------------------
    cluster_flows = {}
    for i, sd in enumerate(station_data):
        label = int(labels[i])
        if label == -1:
            continue
        if label not in cluster_flows:
            cluster_flows[label] = 0
        cluster_flows[label] += sd['total_flow']

    # ä½¿ç”¨å®¢æµé‡ä¸­ä½æ•°åˆ¤æ–­çƒ­ç‚¹
    if cluster_flows:
        flow_values = list(cluster_flows.values())
        flow_median = np.median(flow_values)
        flow_threshold = flow_median * 1.2  # é«˜äºä¸­ä½æ•° 20% ä¸ºçƒ­ç‚¹
        print(f"å®¢æµä¸­ä½æ•°: {flow_median:.0f}, çƒ­ç‚¹é˜ˆå€¼: {flow_threshold:.0f}")
    else:
        flow_threshold = 0

    # ------------------------------------------------------------------
    # 5. å­˜å…¥æ•°æ®åº“
    # ------------------------------------------------------------------
    ClusterResult.objects.all().delete()
    objects_to_create = []

    for i, sd in enumerate(station_data):
        label = int(labels[i])
        is_hot = (label != -1 and cluster_flows.get(label, 0) > flow_threshold)

        objects_to_create.append(ClusterResult(
            station=sd['station'],
            cluster_label=label,
            longitude=sd['lon'],
            latitude=sd['lat'],
            total_flow=sd['total_flow'],
            is_hotspot=is_hot,
            eps=eps,
            min_samples=min_samples,
            analysis_date=date,
        ))

    ClusterResult.objects.bulk_create(objects_to_create)
    print(f"å·²å†™å…¥ {len(objects_to_create)} æ¡èšç±»ç»“æœåˆ°æ•°æ®åº“")

    # ------------------------------------------------------------------
    # 6. è¾“å‡ºç»Ÿè®¡æŠ¥å‘Š
    # ------------------------------------------------------------------
    print("\n" + "-" * 50)
    print("èšç±»ç»Ÿè®¡æŠ¥å‘Š")
    print("-" * 50)

    for cl in sorted(set(labels)):
        members = [station_data[i] for i in range(len(labels)) if labels[i] == cl]
        total = sum(m['total_flow'] for m in members)
        tag = "ğŸ”¥çƒ­ç‚¹" if cl != -1 and cluster_flows.get(cl, 0) > flow_threshold else ""
        if cl == -1:
            tag = "ğŸ”µå™ªå£°"
        station_names = [m['station'].station_name for m in members]
        print(f"\n  Cluster {cl} ({len(members)} ç«™ç‚¹, æ€»å®¢æµ {total}) {tag}")
        print(f"    ç«™ç‚¹: {', '.join(station_names)}")

    result_summary = {
        'n_clusters': n_clusters,
        'n_noise': n_noise,
        'total_stations': len(station_data),
        'cluster_details': {
            int(cl): {
                'count': len([1 for l in labels if l == cl]),
                'total_flow': cluster_flows.get(int(cl), 0) if cl != -1 else sum(
                    station_data[i]['total_flow'] for i in range(len(labels)) if labels[i] == -1
                ),
                'is_hotspot': cl != -1 and cluster_flows.get(int(cl), 0) > flow_threshold,
            }
            for cl in sorted(set(labels))
        }
    }

    print("\nâœ… DBSCAN èšç±»åˆ†æå®Œæˆï¼")
    return result_summary


if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser(description='DBSCAN ç©ºé—´èšç±»åˆ†æ')
    parser.add_argument('--eps', type=float, default=0.008, help='DBSCAN é‚»åŸŸåŠå¾„ (é»˜è®¤ 0.008 â‰ˆ 800m)')
    parser.add_argument('--min-samples', type=int, default=2, help='æœ€å°æ ·æœ¬æ•° (é»˜è®¤ 2)')
    parser.add_argument('--date', type=str, default=None, help='åˆ†ææ—¥æœŸ (å¦‚ 2025-05-01)')
    args = parser.parse_args()

    result = run_dbscan(eps=args.eps, min_samples=args.min_samples, date=args.date)
    if result:
        print(f"\næœ€ç»ˆç»“æœ: {result['n_clusters']} ä¸ªèšç±», {result['n_noise']} ä¸ªå™ªå£°ç‚¹")
