"""
æ™ºèƒ½å‡ºè¡Œæ¨èå¼•æ“
ç”¨æˆ·è¾“å…¥èµ·ç‚¹ç«™ã€ç»ˆç‚¹ç«™ã€å‡ºè¡Œæ—¶é—´ï¼Œç³»ç»ŸåŸºäºå®¢æµæ•°æ®ç”Ÿæˆä¸ªæ€§åŒ–å‡ºè¡Œå»ºè®®

æ¨èç®—æ³•:
1. æœ€ä¼˜æ—¶æ®µæ¨è â€” Z-Score å¼‚å¸¸æ£€æµ‹: å¯¹ OD å„æ—¶æ®µå®¢æµåšæ ‡å‡†åŒ–ï¼Œè¯†åˆ«å¼‚å¸¸é«˜å³°ï¼Œæ¨èä½äºå‡å€¼çš„æ—¶æ®µ
2. èµ·ç‚¹ç«™æ‹¥æŒ¤åˆ†æ â€” æ—¶åºå¯¹æ¯”: æ‰€é€‰æ—¶æ®µ vs å…¨å¤©å‡å€¼/æ ‡å‡†å·®ï¼Œé‡åŒ–æ‹¥æŒ¤ç¨‹åº¦å¹¶ç»™å‡ºå‰åæ—¶æ®µå¯¹æ¯”
3. ç»ˆç‚¹ç«™æ‹¥æŒ¤åˆ†æ â€” åŒä¸Šï¼Œé’ˆå¯¹åˆ°è¾¾ç«™
4. æ›¿ä»£è·¯çº¿æ¨è â€” ä½™å¼¦ç›¸ä¼¼åº¦: æ„å»ºç«™ç‚¹å®¢æµå‘é‡ï¼Œæ‰¾ä¸ç›®çš„ç«™æœ€ç›¸ä¼¼ä½†æ›´ç©ºé—²çš„æ›¿ä»£ç«™ç‚¹
"""
import os
import sys
import math
import numpy as np
from collections import defaultdict

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'backend'))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'transit_system.settings')

import django
django.setup()

from django.db.models import Sum, Avg, Count, F, Q
from analysis.models import (
    BusStation, OdFlow, StationFlowStats, Recommendation
)


# ==================== ç®—æ³•å·¥å…·å‡½æ•° ====================

def z_score_analysis(values):
    """
    Z-Score æ ‡å‡†åŒ–åˆ†æ
    è¿”å›æ¯ä¸ªå€¼çš„ z-scoreï¼Œç”¨äºè¯†åˆ«å¼‚å¸¸é«˜/ä½æ—¶æ®µ
    z = (x - Î¼) / Ïƒ
    """
    arr = np.array(values, dtype=float)
    mean = np.mean(arr)
    std = np.std(arr)
    if std == 0:
        return np.zeros_like(arr), float(mean), float(std)
    z_scores = (arr - mean) / std
    return z_scores, float(mean), float(std)


def cosine_similarity(vec_a, vec_b):
    """
    ä½™å¼¦ç›¸ä¼¼åº¦
    sim(A, B) = (A Â· B) / (||A|| Ã— ||B||)
    """
    a = np.array(vec_a, dtype=float)
    b = np.array(vec_b, dtype=float)
    dot = np.dot(a, b)
    norm = np.linalg.norm(a) * np.linalg.norm(b)
    return float(dot / norm) if norm > 0 else 0.0


def weighted_score(factors):
    """
    åŠ æƒè¯„åˆ†æ¨¡å‹
    factors: list of (value, weight) â€” value âˆˆ [0,1], weight âˆˆ R+
    score = Î£(value_i Ã— weight_i) / Î£(weight_i)
    """
    total_w = sum(w for _, w in factors)
    if total_w == 0:
        return 0.0
    return sum(v * w for v, w in factors) / total_w


def _get_analysis_date():
    """è·å–æœ€æ–°çš„æœ‰æ•°æ®æ—¥æœŸ"""
    return StationFlowStats.objects.order_by('-date').values_list('date', flat=True).first()


# ==================== æ¨èç­–ç•¥ ====================

def recommend_best_time(origin_id, dest_id, travel_hour, analysis_date):
    """
    ç­–ç•¥1: æœ€ä¼˜æ—¶æ®µæ¨è (Z-Score å¼‚å¸¸æ£€æµ‹)

    ç®—æ³•æµç¨‹:
    1. æŸ¥è¯¢è¯¥ OD å¯¹å…¨å¤©å„æ—¶æ®µå®¢æµé‡
    2. å¯¹å®¢æµåºåˆ—åš Z-Score æ ‡å‡†åŒ–
    3. Z > 1.0 çš„æ—¶æ®µæ ‡è®°ä¸ºã€Œé«˜å³°ã€(çº¢è‰²)
    4. Z < -0.5 çš„æ—¶æ®µæ ‡è®°ä¸ºã€Œæ¨èã€(ç»¿è‰²)
    5. ç”¨æˆ·æ‰€é€‰æ—¶æ®µå¦‚æœ Z > 0.5ï¼Œå»ºè®®è°ƒæ•´
    """
    hourly_qs = (
        OdFlow.objects.filter(
            origin_station_id=origin_id,
            destination_station_id=dest_id,
            date=analysis_date,
        )
        .values('hour')
        .annotate(flow=Sum('flow_count'))
        .order_by('hour')
    )
    hourly = list(hourly_qs)
    if not hourly:
        return None

    hours = [h['hour'] for h in hourly]
    flows = [h['flow'] for h in hourly]
    z_scores, mean_flow, std_flow = z_score_analysis(flows)

    # æ‰¾ç”¨æˆ·æ‰€é€‰æ—¶æ®µ
    user_idx = None
    user_flow = 0
    user_z = 0.0
    for i, h in enumerate(hours):
        if h == travel_hour:
            user_idx = i
            user_flow = flows[i]
            user_z = float(z_scores[i])
            break

    # æ‰¾æœ€ä¼˜æ—¶æ®µ (zæœ€ä½) å’Œæœ€å·®æ—¶æ®µ (zæœ€é«˜)
    best_idx = int(np.argmin(z_scores))
    worst_idx = int(np.argmax(z_scores))

    # æ¨èæ—¶æ®µ: z < -0.3 çš„
    recommended_hours = [hours[i] for i in range(len(hours)) if z_scores[i] < -0.3]
    peak_hours = [hours[i] for i in range(len(hours)) if z_scores[i] > 0.8]

    # åˆ¤æ–­ç”¨æˆ·é€‰çš„æ—¶æ®µå¥½ä¸å¥½
    if user_z > 0.8:
        level = 'é«˜å³°'
        advice = f'æ‚¨é€‰æ‹©çš„ {travel_hour}:00 å®¢æµé‡ {user_flow} äººæ¬¡ï¼Œå¤„äºé«˜å³°æ—¶æ®µï¼ˆZ={user_z:.1f}ï¼‰ã€‚'
    elif user_z > 0.3:
        level = 'åé«˜'
        advice = f'æ‚¨é€‰æ‹©çš„ {travel_hour}:00 å®¢æµé‡ {user_flow} äººæ¬¡ï¼Œç•¥é«˜äºå¹³å‡æ°´å¹³ï¼ˆZ={user_z:.1f}ï¼‰ã€‚'
    elif user_z < -0.3:
        level = 'æ¨è'
        advice = f'æ‚¨é€‰æ‹©çš„ {travel_hour}:00 å®¢æµé‡ {user_flow} äººæ¬¡ï¼Œä½äºå¹³å‡æ°´å¹³ï¼ˆZ={user_z:.1f}ï¼‰ï¼Œæ˜¯ä¸é”™çš„é€‰æ‹©ï¼'
    else:
        level = 'æ­£å¸¸'
        advice = f'æ‚¨é€‰æ‹©çš„ {travel_hour}:00 å®¢æµé‡ {user_flow} äººæ¬¡ï¼Œå¤„äºæ­£å¸¸æ°´å¹³ï¼ˆZ={user_z:.1f}ï¼‰ã€‚'

    if recommended_hours and level in ('é«˜å³°', 'åé«˜'):
        rec_str = ', '.join(f'{h}:00' for h in sorted(recommended_hours)[:3])
        advice += f' å»ºè®®æ”¹ä¸º {rec_str} å‡ºè¡Œï¼Œå®¢æµæ›´å°‘ã€‚'

    origin_name = BusStation.objects.filter(pk=origin_id).values_list('station_name', flat=True).first() or origin_id
    dest_name = BusStation.objects.filter(pk=dest_id).values_list('station_name', flat=True).first() or dest_id

    # å‡ºè¡Œé€‚å®œåº¦è¯„åˆ†: è¶Šé€‚åˆå‡ºè¡Œåˆ†æ•°è¶Šé«˜
    score = weighted_score([
        (max(0, min(1.0, 0.5 - user_z * 0.25)), 0.6),  # Zè¶Šä½(è¶Šç©ºé—²)â†’åˆ†è¶Šé«˜
        (1 - min(1.0, std_flow / max(mean_flow, 1)), 0.2),  # æ³¢åŠ¨å°â†’æ›´ç¨³å®šâ†’åˆ†è¶Šé«˜
        (0.8 if level in ('æ¨è', 'æ­£å¸¸') else 0.3, 0.2),  # å½“å‰æ—¶æ®µå·²æ˜¯ä½å³°â†’åˆ†æ›´é«˜
    ])

    return {
        'rec_type': 'time',
        'title': f'â° {origin_name} â†’ {dest_name} æ—¶æ®µåˆ†æ',
        'description': advice,
        'score': round(max(0.05, min(0.98, score)), 2),
        'metadata': {
            'origin': origin_name,
            'destination': dest_name,
            'hourly_flow': {h: f for h, f in zip(hours, flows)},
            'z_scores': {h: round(float(z), 2) for h, z in zip(hours, z_scores)},
            'mean_flow': round(mean_flow, 1),
            'std_flow': round(std_flow, 1),
            'user_hour': travel_hour,
            'user_flow': user_flow,
            'user_z': round(user_z, 2),
            'user_level': level,
            'best_hour': hours[best_idx],
            'worst_hour': hours[worst_idx],
            'recommended_hours': sorted(recommended_hours)[:4],
            'peak_hours': sorted(peak_hours),
            'algorithm': 'Z-Score å¼‚å¸¸æ£€æµ‹',
        },
    }


def analyze_station_congestion(station_id, travel_hour, analysis_date, role='origin'):
    """
    ç­–ç•¥2/3: ç«™ç‚¹æ‹¥æŒ¤åº¦åˆ†æ (æ—¶åºå¯¹æ¯” + Z-Score)

    ç®—æ³•æµç¨‹:
    1. æŸ¥è¯¢è¯¥ç«™å…¨å¤©å„æ—¶æ®µå®¢æµ
    2. Z-Score è¯†åˆ«å¼‚å¸¸æ‹¥æŒ¤æ—¶æ®µ
    3. è®¡ç®—æ‰€é€‰æ—¶æ®µçš„æ‹¥æŒ¤ç™¾åˆ†ä½ (åœ¨å…¨å¤©ä¸­æ’ç¬¬å‡ )
    4. ä¸å‰å Â±2 å°æ—¶å¯¹æ¯”ï¼Œç»™å‡ºæœ€ä¼˜çª—å£
    """
    hourly_qs = (
        StationFlowStats.objects.filter(station_id=station_id, date=analysis_date)
        .values('hour', 'total_flow', 'in_flow', 'out_flow', 'congestion_level')
        .order_by('hour')
    )
    records = list(hourly_qs)
    if not records:
        return None

    station_name = BusStation.objects.filter(pk=station_id).values_list('station_name', flat=True).first() or station_id
    role_label = 'å‡ºå‘ç«™' if role == 'origin' else 'åˆ°è¾¾ç«™'

    hours = [r['hour'] for r in records]
    flows = [r['total_flow'] for r in records]
    z_scores, mean_flow, std_flow = z_score_analysis(flows)

    # æ‰¾åˆ°ç”¨æˆ·æ‰€é€‰æ—¶æ®µæ•°æ®
    user_record = None
    user_z = 0.0
    user_flow = 0
    for i, r in enumerate(records):
        if r['hour'] == travel_hour:
            user_record = r
            user_z = float(z_scores[i])
            user_flow = r['total_flow']
            break

    if not user_record:
        # è¯¥æ—¶æ®µæ— æ•°æ®ï¼Œå–æœ€è¿‘çš„
        user_flow = 0
        congestion = 'unknown'
    else:
        congestion = user_record['congestion_level']

    # ç™¾åˆ†ä½æ’å: è¯¥æ—¶æ®µåœ¨å…¨å¤©ä¸­æ’ç¬¬å‡ 
    sorted_flows = sorted(flows)
    percentile = (sorted_flows.index(user_flow) + 1) / len(sorted_flows) * 100 if user_flow in sorted_flows else 50

    # å‰å Â±2 å°æ—¶çª—å£å¯¹æ¯”
    nearby = {}
    for i, r in enumerate(records):
        if abs(r['hour'] - travel_hour) <= 2:
            nearby[r['hour']] = {
                'flow': r['total_flow'],
                'z': round(float(z_scores[i]), 2),
                'level': r['congestion_level'],
            }

    # æ¨èæœ€ä½³çª—å£
    best_nearby_hour = min(nearby.keys(), key=lambda h: nearby[h]['flow']) if nearby else travel_hour

    # æ‹¥æŒ¤åº¦æ–‡å­—
    if congestion == 'high':
        desc = f'{station_name}ï¼ˆ{role_label}ï¼‰åœ¨ {travel_hour}:00 å®¢æµ {user_flow} äººæ¬¡ï¼Œæ‹¥æŒ¤åº¦ã€é«˜ã€‘ï¼ˆZ={user_z:.1f}ï¼Œè¶…è¿‡å…¨å¤© {percentile:.0f}% çš„æ—¶æ®µï¼‰ã€‚'
    elif congestion == 'medium':
        desc = f'{station_name}ï¼ˆ{role_label}ï¼‰åœ¨ {travel_hour}:00 å®¢æµ {user_flow} äººæ¬¡ï¼Œæ‹¥æŒ¤åº¦ã€ä¸­ç­‰ã€‘ï¼ˆZ={user_z:.1f}ï¼Œè¶…è¿‡å…¨å¤© {percentile:.0f}% çš„æ—¶æ®µï¼‰ã€‚'
    else:
        desc = f'{station_name}ï¼ˆ{role_label}ï¼‰åœ¨ {travel_hour}:00 å®¢æµ {user_flow} äººæ¬¡ï¼Œæ‹¥æŒ¤åº¦ã€ä½ã€‘ï¼ˆZ={user_z:.1f}ï¼‰ï¼Œå‡ºè¡Œè¾ƒä¸ºèˆ’é€‚ã€‚'

    if best_nearby_hour != travel_hour and nearby.get(best_nearby_hour, {}).get('flow', 999999) < user_flow:
        desc += f' é™„è¿‘æ—¶æ®µ {best_nearby_hour}:00 å®¢æµä»… {nearby[best_nearby_hour]["flow"]} äººæ¬¡ï¼Œå¯è€ƒè™‘è°ƒæ•´ã€‚'

    icon = 'ğŸš‰' if role == 'origin' else 'ğŸ'
    # å‡ºè¡Œé€‚å®œåº¦è¯„åˆ†: è¶Šä¸æ‹¥æŒ¤åˆ†æ•°è¶Šé«˜
    score = weighted_score([
        (max(0, min(1.0, 0.5 - user_z * 0.25)), 0.5),  # Zè¶Šä½â†’è¶Šç©ºé—²â†’åˆ†è¶Šé«˜
        (1 - percentile / 100.0, 0.3),  # ç™¾åˆ†ä½è¶Šä½(è¶Šä¸æŒ¤)â†’åˆ†è¶Šé«˜
        (1.0 if congestion == 'low' else 0.5 if congestion == 'medium' else 0.15, 0.2),
    ])

    return {
        'rec_type': 'avoid',
        'title': f'{icon} {role_label} {station_name} æ‹¥æŒ¤åˆ†æ',
        'description': desc,
        'score': round(max(0.05, min(0.98, score)), 2),
        'metadata': {
            'station': station_name,
            'station_id': station_id,
            'role': role_label,
            'hourly_flow': {h: f for h, f in zip(hours, flows)},
            'z_scores': {h: round(float(z), 2) for h, z in zip(hours, z_scores)},
            'user_hour': travel_hour,
            'user_flow': user_flow,
            'user_z': round(user_z, 2),
            'congestion_level': congestion,
            'percentile': round(percentile, 1),
            'nearby_hours': nearby,
            'best_nearby_hour': best_nearby_hour,
            'mean_flow': round(mean_flow, 1),
            'suggested_hours': [h for h in hours if z_scores[hours.index(h)] < -0.3][:4],
            'congested_hours': [h for h in hours if z_scores[hours.index(h)] > 0.8],
            'algorithm': 'Z-Score + ç™¾åˆ†ä½æ’å',
        },
    }


def recommend_alternative_routes(origin_id, dest_id, travel_hour, analysis_date):
    """
    ç­–ç•¥4: æ›¿ä»£è·¯çº¿æ¨è (ä½™å¼¦ç›¸ä¼¼åº¦)

    ç®—æ³•æµç¨‹:
    1. æ„å»ºç›®çš„ç«™çš„å®¢æµç‰¹å¾å‘é‡ V_dest = [hour_6_flow, hour_7_flow, ..., hour_22_flow]
    2. æ„å»ºæ‰€æœ‰å…¶ä»–ç«™çš„å®¢æµç‰¹å¾å‘é‡
    3. ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ sim(V_dest, V_other) æ‰¾åŠŸèƒ½ç›¸ä¼¼çš„ç«™
    4. åœ¨ç›¸ä¼¼ç«™ä¸­ç­›é€‰å‡ºå½“å‰æ—¶æ®µæ›´ç©ºé—²çš„ç«™ç‚¹ä½œä¸ºæ›¿ä»£
    5. åŒæ—¶æ£€æŸ¥æ›¿ä»£ OD æ˜¯å¦å­˜åœ¨å®¢æµæ•°æ®
    """
    # ç›®çš„ç«™å…¨å¤©å®¢æµå‘é‡
    dest_hourly = dict(
        StationFlowStats.objects.filter(station_id=dest_id, date=analysis_date)
        .values_list('hour', 'total_flow')
    )
    if not dest_hourly:
        return None

    all_hours = sorted(dest_hourly.keys())
    dest_vector = [dest_hourly.get(h, 0) for h in all_hours]
    dest_flow_at_hour = dest_hourly.get(travel_hour, 0)

    dest_name = BusStation.objects.filter(pk=dest_id).values_list('station_name', flat=True).first() or dest_id
    origin_name = BusStation.objects.filter(pk=origin_id).values_list('station_name', flat=True).first() or origin_id

    # è·å–æ‰€æœ‰ç«™ç‚¹åœ¨åŒä¸€å¤©çš„å®¢æµ (æ’é™¤èµ·ç‚¹å’Œç»ˆç‚¹)
    all_station_flows = (
        StationFlowStats.objects.filter(date=analysis_date)
        .exclude(station_id__in=[origin_id, dest_id])
        .values('station_id', 'station__station_name', 'hour', 'total_flow')
    )

    # æŒ‰ç«™ç‚¹åˆ†ç»„æ„å»ºå‘é‡
    station_vectors = defaultdict(lambda: {'name': '', 'flows': {}})
    for sf in all_station_flows:
        sid = sf['station_id']
        station_vectors[sid]['name'] = sf['station__station_name']
        station_vectors[sid]['flows'][sf['hour']] = sf['total_flow']

    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¹¶ç­›é€‰
    candidates = []
    for sid, info in station_vectors.items():
        vec = [info['flows'].get(h, 0) for h in all_hours]
        sim = cosine_similarity(dest_vector, vec)

        alt_flow_at_hour = info['flows'].get(travel_hour, 0)

        # åªè¦ç›¸ä¼¼åº¦ > 0.7 ä¸”å½“å‰æ—¶æ®µæ›´ç©ºé—²çš„
        if sim > 0.7 and alt_flow_at_hour < dest_flow_at_hour:
            # æ£€æŸ¥ä» origin åˆ°è¿™ä¸ªæ›¿ä»£ç«™æ˜¯å¦æœ‰ OD æ•°æ®
            od_flow = OdFlow.objects.filter(
                origin_station_id=origin_id,
                destination_station_id=sid,
                date=analysis_date,
                hour=travel_hour,
            ).aggregate(total=Sum('flow_count'))['total'] or 0

            savings = round((1 - alt_flow_at_hour / max(dest_flow_at_hour, 1)) * 100, 1)

            candidates.append({
                'station_id': sid,
                'station_name': info['name'],
                'similarity': round(sim, 3),
                'flow_at_hour': alt_flow_at_hour,
                'od_flow': od_flow,
                'savings': savings,
            })

    # æŒ‰ç›¸ä¼¼åº¦Ã—èŠ‚çœç‡ç»¼åˆæ’åº
    candidates.sort(key=lambda x: x['similarity'] * (x['savings'] / 100), reverse=True)
    top_alts = candidates[:3]

    if not top_alts:
        return None

    alt_desc_parts = []
    for alt in top_alts:
        alt_desc_parts.append(
            f'{alt["station_name"]}ï¼ˆç›¸ä¼¼åº¦ {alt["similarity"]:.0%}ï¼Œ'
            f'{travel_hour}:00 å®¢æµ {alt["flow_at_hour"]} äººæ¬¡ï¼Œå‡å°‘ {alt["savings"]}%ï¼‰'
        )

    desc = (
        f'{dest_name} åœ¨ {travel_hour}:00 å®¢æµ {dest_flow_at_hour} äººæ¬¡ã€‚'
        f'åŸºäºä½™å¼¦ç›¸ä¼¼åº¦ç®—æ³•ï¼Œä¸ºæ‚¨æ‰¾åˆ°ä»¥ä¸‹åŠŸèƒ½ç›¸ä¼¼ä½†æ›´ç©ºé—²çš„æ›¿ä»£ç«™ç‚¹ï¼š\n'
        + 'ï¼›'.join(alt_desc_parts) + 'ã€‚'
    )

    # æ›¿ä»£æ–¹æ¡ˆè´¨é‡è¯„åˆ†: æ›¿ä»£è¶Šä¼˜è´¨åˆ†æ•°è¶Šé«˜
    score = weighted_score([
        (min(1.0, top_alts[0]['savings'] / 50), 0.4),  # èŠ‚çœæ¯”ä¾‹è¶Šé«˜â†’æ–¹æ¡ˆè¶Šå¥½
        (top_alts[0]['similarity'], 0.4),  # ç›¸ä¼¼åº¦è¶Šé«˜â†’æ›¿ä»£è¶Šå¯è¡Œ
        (min(1.0, len(top_alts) / 3), 0.2),  # å¯é€‰æ–¹æ¡ˆè¶Šå¤šè¶Šå¥½
    ])

    return {
        'rec_type': 'route',
        'title': f'ğŸ”„ æ›¿ä»£ç›®çš„ç«™æ¨è',
        'description': desc,
        'score': round(max(0.05, min(0.95, 0.2 + score * 0.6)), 2),
        'metadata': {
            'origin': origin_name,
            'destination': dest_name,
            'dest_flow_at_hour': dest_flow_at_hour,
            'travel_hour': travel_hour,
            'alternatives': top_alts,
            'algorithm': 'ä½™å¼¦ç›¸ä¼¼åº¦ + åŠ æƒè¯„åˆ†',
        },
    }


# ==================== ä¸»å…¥å£ ====================

def generate_od_recommendations(origin_id, dest_id, travel_hour, analysis_date=None):
    """
    æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„ èµ·ç‚¹ã€ç»ˆç‚¹ã€å‡ºè¡Œæ—¶æ®µ ç”Ÿæˆå‡ºè¡Œæ¨èï¼ˆå®æ—¶è®¡ç®—ï¼Œä¸å†™åº“ï¼‰

    ä½¿ç”¨çš„ç®—æ³•:
    - Z-Score å¼‚å¸¸æ£€æµ‹ (æ—¶æ®µæ¨èã€æ‹¥æŒ¤åˆ†æ)
    - ä½™å¼¦ç›¸ä¼¼åº¦ (æ›¿ä»£è·¯çº¿)
    - åŠ æƒè¯„åˆ†æ¨¡å‹ (æ¨èä¼˜å…ˆçº§æ’åº)
    """
    if not analysis_date:
        analysis_date = _get_analysis_date()
    if not analysis_date:
        return []

    results = []

    # ç­–ç•¥1: æœ€ä¼˜æ—¶æ®µæ¨è
    rec = recommend_best_time(origin_id, dest_id, travel_hour, analysis_date)
    if rec:
        results.append(rec)

    # ç­–ç•¥2: èµ·ç‚¹ç«™æ‹¥æŒ¤åˆ†æ
    rec = analyze_station_congestion(origin_id, travel_hour, analysis_date, role='origin')
    if rec:
        results.append(rec)

    # ç­–ç•¥3: ç»ˆç‚¹ç«™æ‹¥æŒ¤åˆ†æ
    rec = analyze_station_congestion(dest_id, travel_hour, analysis_date, role='destination')
    if rec:
        results.append(rec)

    # ç­–ç•¥4: æ›¿ä»£è·¯çº¿æ¨è
    rec = recommend_alternative_routes(origin_id, dest_id, travel_hour, analysis_date)
    if rec:
        results.append(rec)

    # æŒ‰ score æ’åº
    results.sort(key=lambda x: x['score'], reverse=True)

    return results


def import_travel_history_from_od():
    """ä» OD æ•°æ®ä¸­æå–å‡ºè¡Œå†å²ï¼ˆæ¨¡æ‹Ÿï¼‰"""
    import pandas as pd
    from analysis.models import UserTravelHistory

    csv_path = os.path.join(os.path.dirname(__file__), '..', 'data', 'raw', 'mock_swipe_records.csv')
    if not os.path.exists(csv_path):
        print("æœªæ‰¾åˆ°åˆ·å¡æ•°æ®æ–‡ä»¶")
        return

    df = pd.read_csv(csv_path)
    df['swipe_time'] = pd.to_datetime(df['swipe_time'])
    df.sort_values(by=['card_id', 'swipe_time'], inplace=True)

    df['next_card'] = df['card_id'].shift(-1)
    df['next_swipe_type'] = df['swipe_type'].shift(-1)
    df['next_station'] = df['station_name'].shift(-1)
    df['next_time'] = df['swipe_time'].shift(-1)

    trips = df[(df['card_id'] == df['next_card']) &
               (df['swipe_type'] == 'in') &
               (df['next_swipe_type'] == 'out')].copy()

    trips['duration_min'] = (trips['next_time'] - trips['swipe_time']).dt.total_seconds() / 60

    UserTravelHistory.objects.all().delete()
    objects = []
    for _, row in trips.iterrows():
        objects.append(UserTravelHistory(
            user_id=row['card_id'],
            origin_station_id=row['station_name'],
            destination_station_id=row['next_station'],
            travel_date=row['swipe_time'].date(),
            travel_hour=row['swipe_time'].hour,
            duration_min=row['duration_min'],
        ))

    batch_size = 500
    for i in range(0, len(objects), batch_size):
        UserTravelHistory.objects.bulk_create(objects[i:i + batch_size])

    print(f"å·²å¯¼å…¥ {len(objects)} æ¡å‡ºè¡Œå†å²è®°å½•")
    return len(objects)


if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser(description='æ™ºèƒ½å‡ºè¡Œæ¨èå¼•æ“')
    parser.add_argument('--import-history', action='store_true', help='ä»åˆ·å¡æ•°æ®å¯¼å…¥å‡ºè¡Œå†å²')
    parser.add_argument('--origin', type=str, help='èµ·ç‚¹ç«™ID')
    parser.add_argument('--dest', type=str, help='ç»ˆç‚¹ç«™ID')
    parser.add_argument('--hour', type=int, help='å‡ºè¡Œæ—¶æ®µ (0-23)')
    parser.add_argument('--date', type=str, default=None, help='åˆ†ææ—¥æœŸ')
    args = parser.parse_args()

    if args.import_history:
        import_travel_history_from_od()

    if args.origin and args.dest and args.hour is not None:
        recs = generate_od_recommendations(args.origin, args.dest, args.hour, args.date)
        print(f"\nå…±ç”Ÿæˆ {len(recs)} æ¡æ¨è:")
        for r in recs:
            print(f"  [{r['rec_type']}] {r['title']} (score={r['score']:.2f})")
            print(f"    {r['description'][:100]}...")
            print(f"    ç®—æ³•: {r['metadata'].get('algorithm', 'N/A')}")
